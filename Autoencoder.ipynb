{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "All needed imports included here\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 1368\n",
      "Validation Set Size: 236\n",
      "Test Set Size: 383\n",
      "Voxel Dimensions: (1, 32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f64b093d78d43d6a0a4f9e3f7b06318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create data loaders and augmentations needed here\n",
    "\"\"\"\n",
    "from Data.ShapeNetDataLoader import ShapeNetVoxelData\n",
    "from Utils.visualization import visualize_occupancy\n",
    "\n",
    "overfit = False\n",
    "\n",
    "shapenet_core_path = Path(\"D:\\ShapeNetCoreVoxel32\")\n",
    "shapenet_splits_csv_path = Path(\"Data/shapenet_splits.csv\")\n",
    "voxel_filename = \"model_3.binvox\"\n",
    "# load only models from some synsets\n",
    "synset_id_filter = [\"04379243\"]  # tables\n",
    "train_data = ShapeNetVoxelData(shapenet_core_path=shapenet_core_path, shapenet_splits_csv_path=shapenet_splits_csv_path, split=\"train\", \n",
    "    overfit=overfit, synset_id_filter=synset_id_filter, voxel_filename=voxel_filename\n",
    ")\n",
    "print(f\"Train Set Size: {len(train_data)}\")\n",
    "val_data = ShapeNetVoxelData(shapenet_core_path=shapenet_core_path, shapenet_splits_csv_path=shapenet_splits_csv_path, split=\"val\",\n",
    "    overfit=overfit, synset_id_filter=synset_id_filter, voxel_filename=voxel_filename\n",
    ")\n",
    "print(f\"Validation Set Size: {len(val_data)}\")\n",
    "test_data = ShapeNetVoxelData(shapenet_core_path=shapenet_core_path, shapenet_splits_csv_path=shapenet_splits_csv_path, split=\"test\",\n",
    "    overfit=overfit, synset_id_filter=synset_id_filter, voxel_filename=voxel_filename\n",
    ")\n",
    "print(f\"Test Set Size: {len(test_data)}\")\n",
    "\n",
    "train_sample = train_data[0]\n",
    "print(f'Voxel Dimensions: {train_sample.shape}')\n",
    "\n",
    "visualize_occupancy(train_sample.squeeze(), flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purifying predicted Meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCode to purify meshes predicted by the previous networks to be used in the retrieval step\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code to purify meshes predicted by the previous networks to be used in the retrieval step\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesh Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type    | Params\n",
      "-----------------------------------\n",
      "0 | _model | Network | 34.1 M\n",
      "-----------------------------------\n",
      "34.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "34.1 M    Total params\n",
      "136.242   Total estimated model params size (MB)\n",
      "C:\\Users\\svlwe\\miniconda3\\envs\\ml-3d\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:623: UserWarning: Checkpoint directory D:\\Models\\VoxelAutoencoder exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svlwe\\miniconda3\\envs\\ml-3d\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svlwe\\miniconda3\\envs\\ml-3d\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:  12%|█▏        | 3/26 [2:22:04<18:09:15, 2841.56s/it, loss=0.0603, v_num=257]\n",
      "Epoch 293: 100%|██████████| 26/26 [00:07<00:00,  3.58it/s, loss=0.034, v_num=258] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  2253.2         \t|  100 %          \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  7.6579         \t|294            \t|  2251.4         \t|  99.92          \t|\n",
      "run_training_batch                 \t|  0.25499        \t|6468           \t|  1649.3         \t|  73.195         \t|\n",
      "optimizer_step_with_closure_0      \t|  0.12885        \t|6468           \t|  833.39         \t|  36.986         \t|\n",
      "training_step_and_backward         \t|  0.11862        \t|6468           \t|  767.22         \t|  34.05          \t|\n",
      "model_forward                      \t|  0.11149        \t|6468           \t|  721.13         \t|  32.004         \t|\n",
      "training_step                      \t|  0.11122        \t|6468           \t|  719.39         \t|  31.927         \t|\n",
      "get_train_batch                    \t|  0.031393       \t|6762           \t|  212.28         \t|  9.4211         \t|\n",
      "fetch_next_train_batch             \t|  0.031358       \t|6762           \t|  212.05         \t|  9.4107         \t|\n",
      "on_train_epoch_end                 \t|  0.60644        \t|294            \t|  178.29         \t|  7.9128         \t|\n",
      "evaluation_step_and_end            \t|  0.1063         \t|1178           \t|  125.22         \t|  5.5574         \t|\n",
      "validation_step                    \t|  0.10618        \t|1178           \t|  125.08         \t|  5.5511         \t|\n",
      "backward                           \t|  0.0060317      \t|6468           \t|  39.013         \t|  1.7314         \t|\n",
      "get_validate_batch                 \t|  0.025673       \t|1470           \t|  37.74          \t|  1.6749         \t|\n",
      "fetch_next_validate_batch          \t|  0.025642       \t|1470           \t|  37.694         \t|  1.6729         \t|\n",
      "training_batch_to_device           \t|  0.00209        \t|6468           \t|  13.518         \t|  0.59994        \t|\n",
      "on_train_batch_end                 \t|  0.0018956      \t|6468           \t|  12.261         \t|  0.54415        \t|\n",
      "zero_grad                          \t|  0.0010515      \t|6468           \t|  6.801          \t|  0.30183        \t|\n",
      "evaluation_batch_to_device         \t|  0.0020407      \t|1178           \t|  2.404          \t|  0.10669        \t|\n",
      "on_validation_batch_end            \t|  0.0020374      \t|1178           \t|  2.4            \t|  0.10651        \t|\n",
      "on_train_batch_start               \t|  0.00036596     \t|6468           \t|  2.367          \t|  0.10505        \t|\n",
      "on_validation_end                  \t|  0.0043932      \t|295            \t|  1.296          \t|  0.057517       \t|\n",
      "on_validation_start                \t|  0.0042746      \t|295            \t|  1.261          \t|  0.055964       \t|\n",
      "fetch_next_sanity_check_batch      \t|  0.35933        \t|3              \t|  1.078          \t|  0.047842       \t|\n",
      "get_sanity_check_batch             \t|  0.35933        \t|3              \t|  1.078          \t|  0.047842       \t|\n",
      "on_train_epoch_start               \t|  0.0020612      \t|294            \t|  0.606          \t|  0.026895       \t|\n",
      "on_batch_start                     \t|  4.6073e-05     \t|6468           \t|  0.298          \t|  0.013225       \t|\n",
      "training_step_end                  \t|  3.1076e-05     \t|6468           \t|  0.201          \t|  0.0089205      \t|\n",
      "on_before_optimizer_step           \t|  2.8912e-05     \t|6468           \t|  0.187          \t|  0.0082992      \t|\n",
      "on_batch_end                       \t|  2.18e-05       \t|6468           \t|  0.141          \t|  0.0062577      \t|\n",
      "on_before_zero_grad                \t|  2.1645e-05     \t|6468           \t|  0.14           \t|  0.0062133      \t|\n",
      "on_after_backward                  \t|  1.9017e-05     \t|6468           \t|  0.123          \t|  0.0054588      \t|\n",
      "on_validation_model_eval           \t|  0.00032203     \t|295            \t|  0.095          \t|  0.0042162      \t|\n",
      "on_before_backward                 \t|  9.5857e-06     \t|6468           \t|  0.062          \t|  0.0027516      \t|\n",
      "on_sanity_check_start              \t|  0.016          \t|1              \t|  0.016          \t|  0.00071009     \t|\n",
      "on_validation_epoch_start          \t|  5.4237e-05     \t|295            \t|  0.016          \t|  0.00071009     \t|\n",
      "on_validation_batch_start          \t|  1.3582e-05     \t|1178           \t|  0.016          \t|  0.00071009     \t|\n",
      "validation_step_end                \t|  1.3582e-05     \t|1178           \t|  0.016          \t|  0.00071009     \t|\n",
      "on_validation_epoch_end            \t|  5.4237e-05     \t|295            \t|  0.016          \t|  0.00071009     \t|\n",
      "on_train_start                     \t|  0.016          \t|1              \t|  0.016          \t|  0.00071009     \t|\n",
      "on_train_end                       \t|  0.016          \t|1              \t|  0.016          \t|  0.00071009     \t|\n",
      "on_epoch_start                     \t|  2.5467e-05     \t|589            \t|  0.015          \t|  0.00066571     \t|\n",
      "prepare_data                       \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "configure_callbacks                \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_before_accelerator_backend_setup\t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "setup                              \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "configure_sharded_model            \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_configure_sharded_model         \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "configure_optimizers               \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_fit_start                       \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_pretrain_routine_start          \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_pretrain_routine_end            \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_val_dataloader                  \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "val_dataloader                     \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_epoch_end                       \t|  0.0            \t|589            \t|  0.0            \t|  0.0            \t|\n",
      "on_sanity_check_end                \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_train_dataloader                \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "train_dataloader                   \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "on_fit_end                         \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "teardown                           \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%env CUDA_LAUNCH_BLOCKING=1\n",
    "\"\"\"\n",
    "AutoEncoder Models and/or different techniques used to encode the mesh to a smaller dimensions\n",
    "\"\"\"\n",
    "from Networks.VoxelAutoencoder import VoxelAutoencoder\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# lower kl_divergence_scale -> smoother latent space but worse reconstruction\n",
    "# however a higher kl_divergence_scale causes too much overlap between latent space distributions which is impractical for retrieval\n",
    "kl_divergence_scale=0.05\n",
    "latent_dim = 64 # tried 128 which gives slightly better reconstructions, but worse retrieval\n",
    "model = VoxelAutoencoder(train_data, val_data, test_data, device, kl_divergence_scale=kl_divergence_scale, latent_dim=latent_dim)\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"D:/Models/VoxelAutoencoder/\",\n",
    "    filename=\"voxel-autoencoder-01-{epoch:0004d}-{val_loss:.4f}\",\n",
    "    save_top_k=3,\n",
    "    every_n_epochs=8,\n",
    "    mode=\"min\",\n",
    ")\n",
    "tqdm_progess_bar = TQDMProgressBar(refresh_rate=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=32, mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1024,\n",
    "    gpus=1 if torch.cuda.is_available() else None,\n",
    "    log_every_n_steps=1,\n",
    "    logger=logger,\n",
    "    callbacks=[model_checkpoint, tqdm_progess_bar, early_stopping],\n",
    "    profiler=\"simple\"\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6111ea5388da49548fed3f78801ca2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7811096f3ce0449392ec5ade80228ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize reconstruction\n",
    "train_test_sample = train_data[1]\n",
    "\n",
    "visualize_occupancy(train_test_sample.squeeze(), flip_axes=True)\n",
    "\n",
    "sample_tensor = torch.from_numpy(train_test_sample[np.newaxis, :])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    decoded_test = model(sample_tensor)\n",
    "    print(model.encode(sample_tensor)[0].shape)\n",
    "\n",
    "tmp_decoded = decoded_test.clone()\n",
    "tmp_decoded[decoded_test<0.5] = 0\n",
    "tmp_decoded[decoded_test>=0.5] = 1\n",
    "\n",
    "decoded_test_np = tmp_decoded.squeeze().detach().numpy()\n",
    "\n",
    "visualize_occupancy(decoded_test_np, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.1903, -0.6447, -0.1866, -0.5829, -0.1963,  0.2412,  0.1207,  0.0647,\n",
      "        -0.5527, -0.5833, -0.3085, -0.1395,  1.3158,  0.1276, -0.0217, -0.2293,\n",
      "        -0.1428,  2.3342, -1.1617,  0.0449,  0.7585, -1.4124,  0.4898, -0.0788,\n",
      "         0.1833, -0.9383,  0.6033,  0.6963, -0.5918,  0.4943,  1.0437, -0.3674,\n",
      "         0.4529, -1.0308, -1.2869,  0.7310, -1.3705, -0.1673,  0.9079, -0.5481,\n",
      "        -0.1301,  0.1290, -1.0288, -0.8064, -0.4644,  0.6228, -0.7107,  0.1057,\n",
      "        -0.7725, -0.8885, -0.1140, -0.0827, -0.5144, -1.1386,  1.0071, -0.0679,\n",
      "         1.8071, -0.5854, -1.1766, -0.7938, -0.2865, -0.3643,  0.5412, -0.6091]), array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., ..., 0., 0., 0.],\n",
      "         [0., 0., 1., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 1., ..., 0., 0., 0.],\n",
      "         [0., 0., 1., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# compute latent vectors of training samples\n",
    "latent_vectors = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for train_sample in train_data:\n",
    "        sample_tensor = torch.from_numpy(train_sample[np.newaxis, :])\n",
    "        vec = torch.zeros(64)\n",
    "        for i in range(2):\n",
    "            vec += model.encode(sample_tensor)[0]\n",
    "        vec /= 2\n",
    "        latent_vectors[vec] = train_sample\n",
    "\n",
    "for latent_vector in latent_vectors.items():\n",
    "    print(latent_vector)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7a49da399745c798664c56dfb43083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272300195ea74f3c9909602f47952747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c180935e4d4c1da890f1ba5a37a748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object 3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343c515bb472475496027d2405c2c625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute latent vector of test sample\n",
    "test_sample = test_data[42]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # result is stochastic -> can be run multiple times to get different results\n",
    "    sample_tensor = torch.from_numpy(test_sample[np.newaxis, :])\n",
    "    test_latent_vector = torch.zeros(64)\n",
    "    for i in range(2):\n",
    "        test_latent_vector += model.encode(sample_tensor)[0]\n",
    "    test_latent_vector /= 2\n",
    "\n",
    "print(\"Test sample:\")\n",
    "visualize_occupancy(test_sample.squeeze(), flip_axes=True)  \n",
    "\n",
    "# find closest latent vector\n",
    "min_distance = float('inf')\n",
    "best_voxel_match_0 = None\n",
    "best_voxel_match_1 = None\n",
    "best_voxel_match_2 = None\n",
    "for train_latent_vector, train_voxel in latent_vectors.items():\n",
    "    distance = torch.dist(test_latent_vector, train_latent_vector)\n",
    "    if (distance < min_distance):\n",
    "        min_distance = distance\n",
    "        best_voxel_match_2 = best_voxel_match_1\n",
    "        best_voxel_match_1 = best_voxel_match_0\n",
    "        best_voxel_match_0 = train_voxel\n",
    "\n",
    "print(\"Retrieved object 1:\")\n",
    "visualize_occupancy(best_voxel_match_0.squeeze(), flip_axes=True)\n",
    "print(\"Retrieved object 2:\")\n",
    "visualize_occupancy(best_voxel_match_1.squeeze(), flip_axes=True)\n",
    "print(\"Retrieved object 3:\") \n",
    "visualize_occupancy(best_voxel_match_2.squeeze(), flip_axes=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesh Retreival Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models/Techniques to use the previous encoding steps to retreive objects from a specified database\n",
    "\"\"\"\n",
    "# TODO: store latent vector of all shapenet models\n",
    "# TODO: encode voxel and find obj that has the closest latent vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Full Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing the entire pipeline implemented with added visualizations and discussions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "\n",
    "[1]....."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7c2a59d5225d3a31d26be8da3fedd8f4fff04f1963eacdf138e0c401e96b304"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('i2dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
