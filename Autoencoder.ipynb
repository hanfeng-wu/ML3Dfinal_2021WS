{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "All needed imports included here\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 1368\n",
      "Validation Set Size: 236\n",
      "Test Set Size: 383\n",
      "Voxel Dimensions: (1, 32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78a382874f44593a7fd81b4f624c4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create data loaders and augmentations needed here\n",
    "\"\"\"\n",
    "from Data.ShapeNetDataLoader import ShapeNetVoxelData\n",
    "from Utils.visualization import visualize_occupancy\n",
    "\n",
    "overfit = False\n",
    "\n",
    "shapenet_core_path = Path(\"Data/ShapeNetCoreVoxel32\")\n",
    "shapenet_splits_csv_path = Path(\"Data/shapenet_splits.csv\")\n",
    "voxel_filename = \"model_3.binvox\"\n",
    "# load only models from some synsets\n",
    "synset_id_filter = [\"04379243\"]  # tables\n",
    "train_data = ShapeNetVoxelData(shapenet_core_path=shapenet_core_path, shapenet_splits_csv_path=shapenet_splits_csv_path, split=\"train\", \n",
    "    overfit=overfit, synset_id_filter=synset_id_filter, voxel_filename=voxel_filename\n",
    ")\n",
    "print(f\"Train Set Size: {len(train_data)}\")\n",
    "val_data = ShapeNetVoxelData(shapenet_core_path=shapenet_core_path, shapenet_splits_csv_path=shapenet_splits_csv_path, split=\"val\",\n",
    "    overfit=overfit, synset_id_filter=synset_id_filter, voxel_filename=voxel_filename\n",
    ")\n",
    "print(f\"Validation Set Size: {len(val_data)}\")\n",
    "test_data = ShapeNetVoxelData(shapenet_core_path=shapenet_core_path, shapenet_splits_csv_path=shapenet_splits_csv_path, split=\"test\",\n",
    "    overfit=overfit, synset_id_filter=synset_id_filter, voxel_filename=voxel_filename\n",
    ")\n",
    "print(f\"Test Set Size: {len(test_data)}\")\n",
    "\n",
    "train_sample = train_data[0]\n",
    "print(f'Voxel Dimensions: {train_sample.shape}')\n",
    "\n",
    "visualize_occupancy(train_sample.squeeze(), flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "#%env CUDA_LAUNCH_BLOCKING=1\n",
    "\"\"\"\n",
    "AutoEncoder Models and/or different techniques used to encode the mesh to a smaller dimensions\n",
    "\"\"\"\n",
    "from Networks.VoxelAutoencoder import VoxelAutoencoder\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# lower kl_divergence_scale -> smoother latent space but worse reconstruction\n",
    "# however a higher kl_divergence_scale causes too much overlap between latent space distributions which is impractical for retrieval\n",
    "kl_divergence_scale=0.05\n",
    "latent_dim = 64 # tried 128 which gives slightly better reconstructions, but worse retrieval\n",
    "model = VoxelAutoencoder(train_data, val_data, test_data, device, kl_divergence_scale=kl_divergence_scale, latent_dim=latent_dim)\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"Assets/Models/VoxelAutoencoder/\",\n",
    "    filename=\"voxel-autoencoder-02-{epoch:0004d}-{val_loss:.4f}\",\n",
    "    save_top_k=3,\n",
    "    every_n_epochs=8,\n",
    "    mode=\"min\",\n",
    ")\n",
    "tqdm_progess_bar = TQDMProgressBar(refresh_rate=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=32, mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1024,\n",
    "    gpus=1 if torch.cuda.is_available() else None,\n",
    "    log_every_n_steps=1,\n",
    "    logger=logger,\n",
    "    callbacks=[model_checkpoint, tqdm_progess_bar, early_stopping],\n",
    "    profiler=\"simple\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c937c9d74df947619db006b2fdfe5f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffbdbacab454a48943c55e21cd27224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_path = \"Assets/Models/VoxelAutoencoder/voxel-autoencoder-01-epoch=0279-val_loss=0.0346.ckpt\"\n",
    "# best_model_path = model_checkpoint.best_model_path\n",
    "model = VoxelAutoencoder.load_from_checkpoint(\n",
    "    best_model_path, train_set=train_data, val_set=val_data, test_set=test_data, device=device, kl_divergence_scale=kl_divergence_scale, latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "# visualize reconstruction\n",
    "train_test_sample = train_data[1]\n",
    "\n",
    "visualize_occupancy(train_test_sample.squeeze(), flip_axes=True)\n",
    "\n",
    "sample_tensor = torch.from_numpy(train_test_sample[np.newaxis, :])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    decoded_test = model(sample_tensor)\n",
    "    print(model.encode(sample_tensor)[0].shape)\n",
    "\n",
    "tmp_decoded = decoded_test.clone()\n",
    "tmp_decoded[decoded_test<0.5] = 0\n",
    "tmp_decoded[decoded_test>=0.5] = 1\n",
    "\n",
    "decoded_test_np = tmp_decoded.squeeze().detach().numpy()\n",
    "\n",
    "visualize_occupancy(decoded_test_np, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute latent vectors of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for train_sample in train_data:\n",
    "        sample_tensor = torch.from_numpy(train_sample[np.newaxis, :])\n",
    "        vec = torch.zeros(64)\n",
    "        for i in range(4):\n",
    "            vec += model.encode(sample_tensor)[0]\n",
    "        vec /= 4\n",
    "        latent_vectors[vec] = train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c796f0306245bf95b5033c57508ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c7e9a3e1f44ca1aa4e7b8593b266ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72429737560d4564ba6a8a13b561eef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved object 3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c8476257bf461e89e90fb7be4442e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute latent vector of test sample\n",
    "test_sample = train_data[40]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # result is stochastic -> can be run multiple times to get different results\n",
    "    sample_tensor = torch.from_numpy(test_sample[np.newaxis, :])\n",
    "    test_latent_vector = torch.zeros(64)\n",
    "    for i in range(4):\n",
    "        test_latent_vector += model.encode(sample_tensor)[0]\n",
    "    test_latent_vector /= 4\n",
    "\n",
    "print(\"Test sample:\")\n",
    "visualize_occupancy(test_sample.squeeze(), flip_axes=True)  \n",
    "\n",
    "# find closest latent vector\n",
    "min_distance = float('inf')\n",
    "best_voxel_match_0 = None\n",
    "best_voxel_match_1 = None\n",
    "best_voxel_match_2 = None\n",
    "for train_latent_vector, train_voxel in latent_vectors.items():\n",
    "    distance = torch.dist(test_latent_vector, train_latent_vector)\n",
    "    if (distance < min_distance):\n",
    "        min_distance = distance\n",
    "        best_voxel_match_2 = best_voxel_match_1\n",
    "        best_voxel_match_1 = best_voxel_match_0\n",
    "        best_voxel_match_0 = train_voxel\n",
    "\n",
    "print(\"Retrieved object 1:\")\n",
    "visualize_occupancy(best_voxel_match_0.squeeze(), flip_axes=True)\n",
    "print(\"Retrieved object 2:\")\n",
    "visualize_occupancy(best_voxel_match_1.squeeze(), flip_axes=True)\n",
    "print(\"Retrieved object 3:\") \n",
    "visualize_occupancy(best_voxel_match_2.squeeze(), flip_axes=True)   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7c2a59d5225d3a31d26be8da3fedd8f4fff04f1963eacdf138e0c401e96b304"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('i2dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
